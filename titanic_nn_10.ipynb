{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11) (418, 10)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"test.csv\", index_col=0)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  \\\n",
       "PassengerId                     \n",
       "1                   0       3   \n",
       "2                   1       1   \n",
       "3                   1       3   \n",
       "4                   1       1   \n",
       "5                   0       3   \n",
       "\n",
       "                                                          Name     Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
       "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "3                                       Heikkinen, Miss. Laina  female  26.0   \n",
       "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "5                                     Allen, Mr. William Henry    male  35.0   \n",
       "\n",
       "             SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                          \n",
       "1                1      0         A/5 21171   7.2500   NaN        S  \n",
       "2                1      0          PC 17599  71.2833   C85        C  \n",
       "3                0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "4                1      0            113803  53.1000  C123        S  \n",
       "5                0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"Name\", \"Ticket\", \"Cabin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(cols_to_drop, axis= 1, inplace=True)\n",
    "test.drop(cols_to_drop, axis= 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_mapping = {'female':0, 'male':1}\n",
    "train['Sex'] = train['Sex'].map(gender_mapping)\n",
    "test['Sex'] = test['Sex'].map(gender_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_mapping = {'S':0, 'C':1, 'Q': 2}\n",
    "train['Embarked'] = train['Embarked'].map(embarked_mapping)\n",
    "test['Embarked'] = test['Embarked'].map(embarked_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>418.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.265550</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>30.272590</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>35.627188</td>\n",
       "      <td>0.464115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.841838</td>\n",
       "      <td>0.481622</td>\n",
       "      <td>14.181209</td>\n",
       "      <td>0.896760</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>55.907576</td>\n",
       "      <td>0.685516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pclass         Sex         Age       SibSp       Parch        Fare  \\\n",
       "count  418.000000  418.000000  332.000000  418.000000  418.000000  417.000000   \n",
       "mean     2.265550    0.636364   30.272590    0.447368    0.392344   35.627188   \n",
       "std      0.841838    0.481622   14.181209    0.896760    0.981429   55.907576   \n",
       "min      1.000000    0.000000    0.170000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000    0.000000   21.000000    0.000000    0.000000    7.895800   \n",
       "50%      3.000000    1.000000   27.000000    0.000000    0.000000   14.454200   \n",
       "75%      3.000000    1.000000   39.000000    1.000000    0.000000   31.500000   \n",
       "max      3.000000    1.000000   76.000000    8.000000    9.000000  512.329200   \n",
       "\n",
       "         Embarked  \n",
       "count  418.000000  \n",
       "mean     0.464115  \n",
       "std      0.685516  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      2.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Age.fillna(inplace=True, value=test.Age.mean())\n",
    "test.Fare.fillna(inplace=True, value=test.Fare.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.265550</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>30.272590</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>35.627188</td>\n",
       "      <td>0.464115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.841838</td>\n",
       "      <td>0.481622</td>\n",
       "      <td>12.634534</td>\n",
       "      <td>0.896760</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>55.840500</td>\n",
       "      <td>0.685516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.272590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pclass         Sex         Age       SibSp       Parch        Fare  \\\n",
       "count  418.000000  418.000000  418.000000  418.000000  418.000000  418.000000   \n",
       "mean     2.265550    0.636364   30.272590    0.447368    0.392344   35.627188   \n",
       "std      0.841838    0.481622   12.634534    0.896760    0.981429   55.840500   \n",
       "min      1.000000    0.000000    0.170000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000    0.000000   23.000000    0.000000    0.000000    7.895800   \n",
       "50%      3.000000    1.000000   30.272590    0.000000    0.000000   14.454200   \n",
       "75%      3.000000    1.000000   35.750000    1.000000    0.000000   31.500000   \n",
       "max      3.000000    1.000000   76.000000    8.000000    9.000000  512.329200   \n",
       "\n",
       "         Embarked  \n",
       "count  418.000000  \n",
       "mean     0.464115  \n",
       "std      0.685516  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      2.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 8) (418, 7)\n"
     ]
    }
   ],
   "source": [
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.Survived\n",
    "train.drop(\"Survived\", axis= 1, inplace=True)\n",
    "X = train\n",
    "X_final = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16    #we have only 712 elements\n",
    "num_classes = 2    #Survived or not\n",
    "number_of_epochs=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass        3.0000\n",
      "Sex           1.0000\n",
      "Age          80.0000\n",
      "SibSp         8.0000\n",
      "Parch         9.0000\n",
      "Fare        512.3292\n",
      "Embarked      2.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_max = X.append(X_final).max().astype(np.float64)\n",
    "print(X_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X/X.max().astype(np.float64)\n",
    "X_final  = X_final/X.max().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pclass  Sex     Age  SibSp  Parch      Fare  Embarked\n",
       "PassengerId                                                         \n",
       "1            1.000000  1.0  0.2750    0.2    0.0  0.014151       0.0\n",
       "2            0.333333  0.0  0.4750    0.2    0.0  0.139136       0.5\n",
       "3            1.000000  0.0  0.3250    0.0    0.0  0.015469       0.0\n",
       "4            0.333333  0.0  0.4375    0.2    0.0  0.103644       0.0\n",
       "5            1.000000  1.0  0.4375    0.0    0.0  0.015713       0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size= test_size, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 7) (143, 7) (569,) (143,)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(type(X_train), type(X_test), type(y_train), type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(np.array(y_train), num_classes)\n",
    "y_test = keras.utils.to_categorical(np.array(y_test), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (np.matrix(X_train))\n",
    "X_test = (np.matrix(X_test))\n",
    "X_final = (np.matrix(X_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 7) (143, 7) (569, 2) (143, 2)\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'> <class 'numpy.matrixlib.defmatrix.matrix'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(type(X_train), type(X_test), type(y_train), type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instatiaate a model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_featrures = 7\n",
    "number_of_nodes = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add later to it\n",
    "model.add(Dense(70, activation=\"relu\", input_shape=(7,)))\n",
    "model.add(Dense(70, activation=\"relu\", input_shape=(70,)))\n",
    "model.add(Dense(70, activation=\"relu\", input_shape=(70,)))\n",
    "model.add(Dense(70, activation=\"relu\", input_shape=(70,)))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 70)                560       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 70)                4970      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 70)                4970      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 70)                4970      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 142       \n",
      "=================================================================\n",
      "Total params: 15,612\n",
      "Trainable params: 15,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complie the model for running\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer = sgd,\n",
    "             metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "number_of_epochs = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 569 samples, validate on 143 samples\n",
      "Epoch 1/200\n",
      "569/569 [==============================] - 0s 375us/step - loss: 0.6877 - acc: 0.5905 - val_loss: 0.6632 - val_acc: 0.6364\n",
      "Epoch 2/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.6544 - acc: 0.5975 - val_loss: 0.6408 - val_acc: 0.5944\n",
      "Epoch 3/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.6366 - acc: 0.6063 - val_loss: 0.6240 - val_acc: 0.5944\n",
      "Epoch 4/200\n",
      "569/569 [==============================] - 0s 181us/step - loss: 0.6214 - acc: 0.6186 - val_loss: 0.6089 - val_acc: 0.6713\n",
      "Epoch 5/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.6065 - acc: 0.7100 - val_loss: 0.5942 - val_acc: 0.7622\n",
      "Epoch 6/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.5917 - acc: 0.7786 - val_loss: 0.5787 - val_acc: 0.7762\n",
      "Epoch 7/200\n",
      "569/569 [==============================] - 0s 285us/step - loss: 0.5763 - acc: 0.7909 - val_loss: 0.5644 - val_acc: 0.7832\n",
      "Epoch 8/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.5624 - acc: 0.7821 - val_loss: 0.5499 - val_acc: 0.7832\n",
      "Epoch 9/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.5491 - acc: 0.7803 - val_loss: 0.5377 - val_acc: 0.7832\n",
      "Epoch 10/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.5372 - acc: 0.7786 - val_loss: 0.5272 - val_acc: 0.7832\n",
      "Epoch 11/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.5283 - acc: 0.7786 - val_loss: 0.5192 - val_acc: 0.7832\n",
      "Epoch 12/200\n",
      "569/569 [==============================] - 0s 285us/step - loss: 0.5210 - acc: 0.7786 - val_loss: 0.5124 - val_acc: 0.7832\n",
      "Epoch 13/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.5150 - acc: 0.7786 - val_loss: 0.5073 - val_acc: 0.7832\n",
      "Epoch 14/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.5099 - acc: 0.7786 - val_loss: 0.5020 - val_acc: 0.7832\n",
      "Epoch 15/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.5069 - acc: 0.7786 - val_loss: 0.4980 - val_acc: 0.7832\n",
      "Epoch 16/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.5026 - acc: 0.7786 - val_loss: 0.4943 - val_acc: 0.7832\n",
      "Epoch 17/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4984 - acc: 0.7786 - val_loss: 0.4916 - val_acc: 0.7832\n",
      "Epoch 18/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4960 - acc: 0.7786 - val_loss: 0.4877 - val_acc: 0.7832\n",
      "Epoch 19/200\n",
      "569/569 [==============================] - 0s 287us/step - loss: 0.4933 - acc: 0.7803 - val_loss: 0.4846 - val_acc: 0.7832\n",
      "Epoch 20/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4903 - acc: 0.7838 - val_loss: 0.4816 - val_acc: 0.7902\n",
      "Epoch 21/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4875 - acc: 0.7873 - val_loss: 0.4792 - val_acc: 0.7832\n",
      "Epoch 22/200\n",
      "569/569 [==============================] - 0s 182us/step - loss: 0.4853 - acc: 0.7838 - val_loss: 0.4768 - val_acc: 0.7902\n",
      "Epoch 23/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4843 - acc: 0.7891 - val_loss: 0.4737 - val_acc: 0.7902\n",
      "Epoch 24/200\n",
      "569/569 [==============================] - 0s 284us/step - loss: 0.4818 - acc: 0.7891 - val_loss: 0.4718 - val_acc: 0.7902\n",
      "Epoch 25/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4790 - acc: 0.7909 - val_loss: 0.4701 - val_acc: 0.7902\n",
      "Epoch 26/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4765 - acc: 0.7909 - val_loss: 0.4692 - val_acc: 0.7832\n",
      "Epoch 27/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4748 - acc: 0.7873 - val_loss: 0.4637 - val_acc: 0.7902\n",
      "Epoch 28/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4742 - acc: 0.7909 - val_loss: 0.4616 - val_acc: 0.7902\n",
      "Epoch 29/200\n",
      "569/569 [==============================] - 0s 183us/step - loss: 0.4732 - acc: 0.7944 - val_loss: 0.4593 - val_acc: 0.7902\n",
      "Epoch 30/200\n",
      "569/569 [==============================] - 0s 285us/step - loss: 0.4713 - acc: 0.7926 - val_loss: 0.4578 - val_acc: 0.7902\n",
      "Epoch 31/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4708 - acc: 0.7909 - val_loss: 0.4557 - val_acc: 0.7902\n",
      "Epoch 32/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4684 - acc: 0.7909 - val_loss: 0.4551 - val_acc: 0.7902\n",
      "Epoch 33/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4671 - acc: 0.7909 - val_loss: 0.4518 - val_acc: 0.7902\n",
      "Epoch 34/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4653 - acc: 0.7909 - val_loss: 0.4511 - val_acc: 0.7902\n",
      "Epoch 35/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4640 - acc: 0.7926 - val_loss: 0.4500 - val_acc: 0.7902\n",
      "Epoch 36/200\n",
      "569/569 [==============================] - 0s 285us/step - loss: 0.4622 - acc: 0.7961 - val_loss: 0.4467 - val_acc: 0.7902\n",
      "Epoch 37/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4618 - acc: 0.7909 - val_loss: 0.4474 - val_acc: 0.7902\n",
      "Epoch 38/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4599 - acc: 0.7909 - val_loss: 0.4494 - val_acc: 0.7902\n",
      "Epoch 39/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4599 - acc: 0.7891 - val_loss: 0.4413 - val_acc: 0.7902\n",
      "Epoch 40/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4583 - acc: 0.7944 - val_loss: 0.4397 - val_acc: 0.7902\n",
      "Epoch 41/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4583 - acc: 0.7926 - val_loss: 0.4385 - val_acc: 0.7902\n",
      "Epoch 42/200\n",
      "569/569 [==============================] - 0s 287us/step - loss: 0.4585 - acc: 0.7909 - val_loss: 0.4373 - val_acc: 0.7832\n",
      "Epoch 43/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4540 - acc: 0.8032 - val_loss: 0.4439 - val_acc: 0.7902\n",
      "Epoch 44/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4575 - acc: 0.7944 - val_loss: 0.4346 - val_acc: 0.7902\n",
      "Epoch 45/200\n",
      "569/569 [==============================] - 0s 180us/step - loss: 0.4529 - acc: 0.7926 - val_loss: 0.4380 - val_acc: 0.8182\n",
      "Epoch 46/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4531 - acc: 0.7873 - val_loss: 0.4376 - val_acc: 0.7902\n",
      "Epoch 47/200\n",
      "569/569 [==============================] - 0s 298us/step - loss: 0.4547 - acc: 0.7961 - val_loss: 0.4317 - val_acc: 0.7902\n",
      "Epoch 48/200\n",
      "569/569 [==============================] - 0s 204us/step - loss: 0.4520 - acc: 0.7909 - val_loss: 0.4307 - val_acc: 0.7972\n",
      "Epoch 49/200\n",
      "569/569 [==============================] - 0s 212us/step - loss: 0.4524 - acc: 0.7944 - val_loss: 0.4336 - val_acc: 0.7902\n",
      "Epoch 50/200\n",
      "569/569 [==============================] - 0s 279us/step - loss: 0.4483 - acc: 0.7961 - val_loss: 0.4336 - val_acc: 0.8322\n",
      "Epoch 51/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4496 - acc: 0.7979 - val_loss: 0.4271 - val_acc: 0.7972\n",
      "Epoch 52/200\n",
      "569/569 [==============================] - 0s 196us/step - loss: 0.4492 - acc: 0.7926 - val_loss: 0.4265 - val_acc: 0.8182\n",
      "Epoch 53/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4492 - acc: 0.8014 - val_loss: 0.4247 - val_acc: 0.7972\n",
      "Epoch 54/200\n",
      "569/569 [==============================] - 0s 182us/step - loss: 0.4506 - acc: 0.8014 - val_loss: 0.4243 - val_acc: 0.7972\n",
      "Epoch 55/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4428 - acc: 0.7996 - val_loss: 0.4401 - val_acc: 0.7832\n",
      "Epoch 56/200\n",
      "569/569 [==============================] - 0s 282us/step - loss: 0.4438 - acc: 0.7873 - val_loss: 0.4243 - val_acc: 0.8182\n",
      "Epoch 57/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4464 - acc: 0.8084 - val_loss: 0.4222 - val_acc: 0.7972\n",
      "Epoch 58/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4444 - acc: 0.8014 - val_loss: 0.4237 - val_acc: 0.7902\n",
      "Epoch 59/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4449 - acc: 0.8014 - val_loss: 0.4222 - val_acc: 0.7902\n",
      "Epoch 60/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4434 - acc: 0.7944 - val_loss: 0.4192 - val_acc: 0.8042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4432 - acc: 0.8067 - val_loss: 0.4202 - val_acc: 0.8252\n",
      "Epoch 62/200\n",
      "569/569 [==============================] - 0s 281us/step - loss: 0.4434 - acc: 0.7961 - val_loss: 0.4191 - val_acc: 0.7972\n",
      "Epoch 63/200\n",
      "569/569 [==============================] - 0s 183us/step - loss: 0.4418 - acc: 0.8084 - val_loss: 0.4171 - val_acc: 0.7972\n",
      "Epoch 64/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4420 - acc: 0.8032 - val_loss: 0.4158 - val_acc: 0.8182\n",
      "Epoch 65/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4409 - acc: 0.8067 - val_loss: 0.4214 - val_acc: 0.7902\n",
      "Epoch 66/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4425 - acc: 0.8014 - val_loss: 0.4147 - val_acc: 0.8182\n",
      "Epoch 67/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4404 - acc: 0.7979 - val_loss: 0.4149 - val_acc: 0.8042\n",
      "Epoch 68/200\n",
      "569/569 [==============================] - 0s 286us/step - loss: 0.4390 - acc: 0.8102 - val_loss: 0.4152 - val_acc: 0.8042\n",
      "Epoch 69/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4386 - acc: 0.8084 - val_loss: 0.4136 - val_acc: 0.8252\n",
      "Epoch 70/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4383 - acc: 0.8049 - val_loss: 0.4113 - val_acc: 0.8182\n",
      "Epoch 71/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4397 - acc: 0.7996 - val_loss: 0.4110 - val_acc: 0.8182\n",
      "Epoch 72/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4366 - acc: 0.8102 - val_loss: 0.4189 - val_acc: 0.7832\n",
      "Epoch 73/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4358 - acc: 0.8032 - val_loss: 0.4145 - val_acc: 0.8252\n",
      "Epoch 74/200\n",
      "569/569 [==============================] - 0s 287us/step - loss: 0.4378 - acc: 0.7944 - val_loss: 0.4090 - val_acc: 0.8182\n",
      "Epoch 75/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4375 - acc: 0.7944 - val_loss: 0.4091 - val_acc: 0.8182\n",
      "Epoch 76/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4363 - acc: 0.8049 - val_loss: 0.4143 - val_acc: 0.7902\n",
      "Epoch 77/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4363 - acc: 0.8032 - val_loss: 0.4069 - val_acc: 0.8252\n",
      "Epoch 78/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4341 - acc: 0.8067 - val_loss: 0.4067 - val_acc: 0.8322\n",
      "Epoch 79/200\n",
      "569/569 [==============================] - 0s 180us/step - loss: 0.4352 - acc: 0.8102 - val_loss: 0.4067 - val_acc: 0.8322\n",
      "Epoch 80/200\n",
      "569/569 [==============================] - 0s 280us/step - loss: 0.4333 - acc: 0.7979 - val_loss: 0.4065 - val_acc: 0.8182\n",
      "Epoch 81/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4331 - acc: 0.8014 - val_loss: 0.4068 - val_acc: 0.8112\n",
      "Epoch 82/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4338 - acc: 0.8067 - val_loss: 0.4080 - val_acc: 0.8112\n",
      "Epoch 83/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4280 - acc: 0.8137 - val_loss: 0.4127 - val_acc: 0.8042\n",
      "Epoch 84/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4351 - acc: 0.8102 - val_loss: 0.4046 - val_acc: 0.8322\n",
      "Epoch 85/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4293 - acc: 0.8155 - val_loss: 0.4276 - val_acc: 0.8042\n",
      "Epoch 86/200\n",
      "569/569 [==============================] - 0s 290us/step - loss: 0.4343 - acc: 0.8102 - val_loss: 0.4039 - val_acc: 0.8322\n",
      "Epoch 87/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4314 - acc: 0.8120 - val_loss: 0.4035 - val_acc: 0.8322\n",
      "Epoch 88/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4317 - acc: 0.8032 - val_loss: 0.4025 - val_acc: 0.8112\n",
      "Epoch 89/200\n",
      "569/569 [==============================] - 0s 183us/step - loss: 0.4299 - acc: 0.8155 - val_loss: 0.4032 - val_acc: 0.8322\n",
      "Epoch 90/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4335 - acc: 0.8120 - val_loss: 0.4028 - val_acc: 0.8252\n",
      "Epoch 91/200\n",
      "569/569 [==============================] - 0s 282us/step - loss: 0.4318 - acc: 0.7996 - val_loss: 0.4080 - val_acc: 0.8182\n",
      "Epoch 92/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4308 - acc: 0.8084 - val_loss: 0.4028 - val_acc: 0.8322\n",
      "Epoch 93/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4300 - acc: 0.8190 - val_loss: 0.4077 - val_acc: 0.8182\n",
      "Epoch 94/200\n",
      "569/569 [==============================] - 0s 192us/step - loss: 0.4312 - acc: 0.8155 - val_loss: 0.4105 - val_acc: 0.8252\n",
      "Epoch 95/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4315 - acc: 0.8084 - val_loss: 0.4014 - val_acc: 0.8182\n",
      "Epoch 96/200\n",
      "569/569 [==============================] - 0s 183us/step - loss: 0.4282 - acc: 0.8137 - val_loss: 0.4018 - val_acc: 0.8322\n",
      "Epoch 97/200\n",
      "569/569 [==============================] - 0s 286us/step - loss: 0.4328 - acc: 0.8067 - val_loss: 0.4004 - val_acc: 0.8322\n",
      "Epoch 98/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4309 - acc: 0.8067 - val_loss: 0.4013 - val_acc: 0.8322\n",
      "Epoch 99/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4309 - acc: 0.8067 - val_loss: 0.4014 - val_acc: 0.8182\n",
      "Epoch 100/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4274 - acc: 0.8155 - val_loss: 0.4005 - val_acc: 0.8252\n",
      "Epoch 101/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4274 - acc: 0.8014 - val_loss: 0.4003 - val_acc: 0.8252\n",
      "Epoch 102/200\n",
      "569/569 [==============================] - 0s 288us/step - loss: 0.4291 - acc: 0.8172 - val_loss: 0.4001 - val_acc: 0.8252\n",
      "Epoch 103/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4271 - acc: 0.8172 - val_loss: 0.4012 - val_acc: 0.8252\n",
      "Epoch 104/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4274 - acc: 0.8155 - val_loss: 0.4021 - val_acc: 0.8182\n",
      "Epoch 105/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4285 - acc: 0.8172 - val_loss: 0.4036 - val_acc: 0.8322\n",
      "Epoch 106/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4283 - acc: 0.8172 - val_loss: 0.4001 - val_acc: 0.8182\n",
      "Epoch 107/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4268 - acc: 0.8243 - val_loss: 0.4004 - val_acc: 0.8182\n",
      "Epoch 108/200\n",
      "569/569 [==============================] - 0s 286us/step - loss: 0.4262 - acc: 0.8190 - val_loss: 0.4021 - val_acc: 0.8182\n",
      "Epoch 109/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4289 - acc: 0.8207 - val_loss: 0.3999 - val_acc: 0.8322\n",
      "Epoch 110/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4262 - acc: 0.8260 - val_loss: 0.4092 - val_acc: 0.8182\n",
      "Epoch 111/200\n",
      "569/569 [==============================] - 0s 193us/step - loss: 0.4268 - acc: 0.8295 - val_loss: 0.4004 - val_acc: 0.8252\n",
      "Epoch 112/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4267 - acc: 0.8278 - val_loss: 0.3996 - val_acc: 0.8392\n",
      "Epoch 113/200\n",
      "569/569 [==============================] - 0s 282us/step - loss: 0.4257 - acc: 0.8225 - val_loss: 0.4029 - val_acc: 0.8252\n",
      "Epoch 114/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4240 - acc: 0.8049 - val_loss: 0.4094 - val_acc: 0.8042\n",
      "Epoch 115/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4241 - acc: 0.8155 - val_loss: 0.4173 - val_acc: 0.8112\n",
      "Epoch 116/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4272 - acc: 0.8190 - val_loss: 0.3998 - val_acc: 0.8112\n",
      "Epoch 117/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4251 - acc: 0.8137 - val_loss: 0.4117 - val_acc: 0.8252\n",
      "Epoch 118/200\n",
      "569/569 [==============================] - 0s 283us/step - loss: 0.4236 - acc: 0.8278 - val_loss: 0.3999 - val_acc: 0.8322\n",
      "Epoch 119/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4249 - acc: 0.8207 - val_loss: 0.3997 - val_acc: 0.8112\n",
      "Epoch 120/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4238 - acc: 0.8243 - val_loss: 0.3985 - val_acc: 0.8322\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 0s 186us/step - loss: 0.4259 - acc: 0.8155 - val_loss: 0.3988 - val_acc: 0.8182\n",
      "Epoch 122/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4245 - acc: 0.8172 - val_loss: 0.4012 - val_acc: 0.8252\n",
      "Epoch 123/200\n",
      "569/569 [==============================] - 0s 207us/step - loss: 0.4264 - acc: 0.8225 - val_loss: 0.4083 - val_acc: 0.8112\n",
      "Epoch 124/200\n",
      "569/569 [==============================] - 0s 265us/step - loss: 0.4243 - acc: 0.8190 - val_loss: 0.4073 - val_acc: 0.8112\n",
      "Epoch 125/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4256 - acc: 0.8207 - val_loss: 0.4021 - val_acc: 0.8182\n",
      "Epoch 126/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4245 - acc: 0.8260 - val_loss: 0.4088 - val_acc: 0.8252\n",
      "Epoch 127/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4230 - acc: 0.8190 - val_loss: 0.4103 - val_acc: 0.8042\n",
      "Epoch 128/200\n",
      "569/569 [==============================] - 0s 192us/step - loss: 0.4255 - acc: 0.8190 - val_loss: 0.4019 - val_acc: 0.8182\n",
      "Epoch 129/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4234 - acc: 0.8190 - val_loss: 0.3982 - val_acc: 0.8392\n",
      "Epoch 130/200\n",
      "569/569 [==============================] - 0s 280us/step - loss: 0.4219 - acc: 0.8278 - val_loss: 0.3987 - val_acc: 0.8182\n",
      "Epoch 131/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4241 - acc: 0.8225 - val_loss: 0.3983 - val_acc: 0.8252\n",
      "Epoch 132/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4211 - acc: 0.8207 - val_loss: 0.4013 - val_acc: 0.8322\n",
      "Epoch 133/200\n",
      "569/569 [==============================] - 0s 191us/step - loss: 0.4205 - acc: 0.8137 - val_loss: 0.4059 - val_acc: 0.8252\n",
      "Epoch 134/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4214 - acc: 0.8243 - val_loss: 0.4041 - val_acc: 0.8322\n",
      "Epoch 135/200\n",
      "569/569 [==============================] - 0s 287us/step - loss: 0.4186 - acc: 0.8155 - val_loss: 0.4138 - val_acc: 0.8182\n",
      "Epoch 136/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4232 - acc: 0.8243 - val_loss: 0.3979 - val_acc: 0.8322\n",
      "Epoch 137/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4199 - acc: 0.8313 - val_loss: 0.3979 - val_acc: 0.8252\n",
      "Epoch 138/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4192 - acc: 0.8260 - val_loss: 0.4104 - val_acc: 0.8182\n",
      "Epoch 139/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4224 - acc: 0.8155 - val_loss: 0.4040 - val_acc: 0.8252\n",
      "Epoch 140/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4193 - acc: 0.8207 - val_loss: 0.3979 - val_acc: 0.8252\n",
      "Epoch 141/200\n",
      "569/569 [==============================] - 0s 286us/step - loss: 0.4194 - acc: 0.8207 - val_loss: 0.4002 - val_acc: 0.8112\n",
      "Epoch 142/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4204 - acc: 0.8260 - val_loss: 0.3984 - val_acc: 0.8392\n",
      "Epoch 143/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4181 - acc: 0.8225 - val_loss: 0.3975 - val_acc: 0.8322\n",
      "Epoch 144/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4107 - acc: 0.8260 - val_loss: 0.4797 - val_acc: 0.7762\n",
      "Epoch 145/200\n",
      "569/569 [==============================] - 0s 192us/step - loss: 0.4228 - acc: 0.8348 - val_loss: 0.4013 - val_acc: 0.8252\n",
      "Epoch 146/200\n",
      "569/569 [==============================] - 0s 286us/step - loss: 0.4171 - acc: 0.8243 - val_loss: 0.4081 - val_acc: 0.8322\n",
      "Epoch 147/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4190 - acc: 0.8313 - val_loss: 0.3978 - val_acc: 0.8322\n",
      "Epoch 148/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4139 - acc: 0.8225 - val_loss: 0.4340 - val_acc: 0.8112\n",
      "Epoch 149/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4181 - acc: 0.8190 - val_loss: 0.3987 - val_acc: 0.8322\n",
      "Epoch 150/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4140 - acc: 0.8295 - val_loss: 0.3974 - val_acc: 0.8252\n",
      "Epoch 151/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4153 - acc: 0.8190 - val_loss: 0.3976 - val_acc: 0.8322\n",
      "Epoch 152/200\n",
      "569/569 [==============================] - 0s 285us/step - loss: 0.4151 - acc: 0.8260 - val_loss: 0.4020 - val_acc: 0.8252\n",
      "Epoch 153/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4122 - acc: 0.8348 - val_loss: 0.4015 - val_acc: 0.8392\n",
      "Epoch 154/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4173 - acc: 0.8243 - val_loss: 0.3979 - val_acc: 0.8392\n",
      "Epoch 155/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4170 - acc: 0.8243 - val_loss: 0.4050 - val_acc: 0.8252\n",
      "Epoch 156/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4155 - acc: 0.8207 - val_loss: 0.4021 - val_acc: 0.8252\n",
      "Epoch 157/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4151 - acc: 0.8243 - val_loss: 0.3952 - val_acc: 0.8392\n",
      "Epoch 158/200\n",
      "569/569 [==============================] - 0s 290us/step - loss: 0.4137 - acc: 0.8401 - val_loss: 0.3961 - val_acc: 0.8322\n",
      "Epoch 159/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4148 - acc: 0.8190 - val_loss: 0.3957 - val_acc: 0.8252\n",
      "Epoch 160/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4143 - acc: 0.8190 - val_loss: 0.4097 - val_acc: 0.8322\n",
      "Epoch 161/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4189 - acc: 0.8207 - val_loss: 0.3965 - val_acc: 0.8392\n",
      "Epoch 162/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4155 - acc: 0.8225 - val_loss: 0.4091 - val_acc: 0.8182\n",
      "Epoch 163/200\n",
      "569/569 [==============================] - 0s 289us/step - loss: 0.4147 - acc: 0.8330 - val_loss: 0.3995 - val_acc: 0.8322\n",
      "Epoch 164/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4090 - acc: 0.8330 - val_loss: 0.4025 - val_acc: 0.8252\n",
      "Epoch 165/200\n",
      "569/569 [==============================] - 0s 182us/step - loss: 0.4165 - acc: 0.8260 - val_loss: 0.3956 - val_acc: 0.8392\n",
      "Epoch 166/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4124 - acc: 0.8313 - val_loss: 0.4071 - val_acc: 0.8182\n",
      "Epoch 167/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4103 - acc: 0.8295 - val_loss: 0.3987 - val_acc: 0.8462\n",
      "Epoch 168/200\n",
      "569/569 [==============================] - 0s 189us/step - loss: 0.4126 - acc: 0.8243 - val_loss: 0.3969 - val_acc: 0.8252\n",
      "Epoch 169/200\n",
      "569/569 [==============================] - 0s 288us/step - loss: 0.4123 - acc: 0.8278 - val_loss: 0.3968 - val_acc: 0.8392\n",
      "Epoch 170/200\n",
      "569/569 [==============================] - 0s 182us/step - loss: 0.4111 - acc: 0.8278 - val_loss: 0.4207 - val_acc: 0.8182\n",
      "Epoch 171/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4114 - acc: 0.8225 - val_loss: 0.3993 - val_acc: 0.8252\n",
      "Epoch 172/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4104 - acc: 0.8330 - val_loss: 0.3960 - val_acc: 0.8392\n",
      "Epoch 173/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4125 - acc: 0.8225 - val_loss: 0.3956 - val_acc: 0.8462\n",
      "Epoch 174/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4076 - acc: 0.8260 - val_loss: 0.4019 - val_acc: 0.8462\n",
      "Epoch 175/200\n",
      "569/569 [==============================] - 0s 282us/step - loss: 0.4072 - acc: 0.8383 - val_loss: 0.4001 - val_acc: 0.8462\n",
      "Epoch 176/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4113 - acc: 0.8243 - val_loss: 0.3931 - val_acc: 0.8462\n",
      "Epoch 177/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4062 - acc: 0.8225 - val_loss: 0.3969 - val_acc: 0.8252\n",
      "Epoch 178/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4069 - acc: 0.8436 - val_loss: 0.3947 - val_acc: 0.8392\n",
      "Epoch 179/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4068 - acc: 0.8260 - val_loss: 0.3969 - val_acc: 0.8462\n",
      "Epoch 180/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4057 - acc: 0.8225 - val_loss: 0.3958 - val_acc: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "569/569 [==============================] - 0s 282us/step - loss: 0.4058 - acc: 0.8278 - val_loss: 0.3999 - val_acc: 0.8462\n",
      "Epoch 182/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4063 - acc: 0.8190 - val_loss: 0.4091 - val_acc: 0.8042\n",
      "Epoch 183/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4047 - acc: 0.8383 - val_loss: 0.4161 - val_acc: 0.8112\n",
      "Epoch 184/200\n",
      "569/569 [==============================] - 0s 186us/step - loss: 0.4068 - acc: 0.8260 - val_loss: 0.3987 - val_acc: 0.8182\n",
      "Epoch 185/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4056 - acc: 0.8313 - val_loss: 0.4148 - val_acc: 0.8042\n",
      "Epoch 186/200\n",
      "569/569 [==============================] - 0s 283us/step - loss: 0.4034 - acc: 0.8383 - val_loss: 0.3952 - val_acc: 0.8392\n",
      "Epoch 187/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4068 - acc: 0.8207 - val_loss: 0.3989 - val_acc: 0.8252\n",
      "Epoch 188/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4085 - acc: 0.8330 - val_loss: 0.4022 - val_acc: 0.8112\n",
      "Epoch 189/200\n",
      "569/569 [==============================] - 0s 188us/step - loss: 0.4053 - acc: 0.8348 - val_loss: 0.3961 - val_acc: 0.8322\n",
      "Epoch 190/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4075 - acc: 0.8260 - val_loss: 0.3969 - val_acc: 0.8182\n",
      "Epoch 191/200\n",
      "569/569 [==============================] - 0s 190us/step - loss: 0.4025 - acc: 0.8348 - val_loss: 0.3958 - val_acc: 0.8392\n",
      "Epoch 192/200\n",
      "569/569 [==============================] - 0s 283us/step - loss: 0.4039 - acc: 0.8278 - val_loss: 0.3987 - val_acc: 0.8392\n",
      "Epoch 193/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4020 - acc: 0.8366 - val_loss: 0.4016 - val_acc: 0.8112\n",
      "Epoch 194/200\n",
      "569/569 [==============================] - 0s 187us/step - loss: 0.4020 - acc: 0.8330 - val_loss: 0.4001 - val_acc: 0.8462\n",
      "Epoch 195/200\n",
      "569/569 [==============================] - 0s 183us/step - loss: 0.4047 - acc: 0.8243 - val_loss: 0.3966 - val_acc: 0.8322\n",
      "Epoch 196/200\n",
      "569/569 [==============================] - 0s 195us/step - loss: 0.4014 - acc: 0.8278 - val_loss: 0.3971 - val_acc: 0.8322\n",
      "Epoch 197/200\n",
      "569/569 [==============================] - 0s 184us/step - loss: 0.4048 - acc: 0.8366 - val_loss: 0.4087 - val_acc: 0.8252\n",
      "Epoch 198/200\n",
      "569/569 [==============================] - 0s 286us/step - loss: 0.4028 - acc: 0.8383 - val_loss: 0.3987 - val_acc: 0.8252\n",
      "Epoch 199/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.4061 - acc: 0.8330 - val_loss: 0.3978 - val_acc: 0.8392\n",
      "Epoch 200/200\n",
      "569/569 [==============================] - 0s 185us/step - loss: 0.3967 - acc: 0.8330 - val_loss: 0.3980 - val_acc: 0.8182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc55539e898>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size= batch_size, epochs=number_of_epochs, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 0.397956008678\n",
      "Test Accuracy 0.818181819015\n"
     ]
    }
   ],
   "source": [
    "print('Test Loss', score[0])\n",
    "print('Test Accuracy', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re build the architecure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.matrixlib.defmatrix.matrix'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_scaled = np.matrix(X_scaled)\n",
    "y_scaled = keras.utils.to_categorical(np.array(y), num_classes)\n",
    "print(type(X_scaled), type(y_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instatiaate a model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add later to it\n",
    "model.add(Dense(70, activation=\"relu\", input_shape=(7,)))\n",
    "model.add(Dense(70, activation=\"relu\", input_shape=(70,)))\n",
    "model.add(Dense(70, activation=\"relu\", input_shape=(70,)))\n",
    "model.add(Dense(70, activation=\"relu\", input_shape=(70,)))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 70)                560       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 70)                4970      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 70)                4970      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 70)                4970      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 142       \n",
      "=================================================================\n",
      "Total params: 15,612\n",
      "Trainable params: 15,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr= 0.01)\n",
    "\n",
    "#Complie the model for running\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer = sgd,\n",
    "             metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = number_of_epochs + 40\n",
    "print(number_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n",
      "712/712 [==============================] - 0s 322us/step - loss: 0.6745 - acc: 0.6110\n",
      "Epoch 2/240\n",
      "712/712 [==============================] - 0s 166us/step - loss: 0.6409 - acc: 0.6096\n",
      "Epoch 3/240\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6155 - acc: 0.6854\n",
      "Epoch 4/240\n",
      "712/712 [==============================] - 0s 238us/step - loss: 0.5931 - acc: 0.7739\n",
      "Epoch 5/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.5738 - acc: 0.7781\n",
      "Epoch 6/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.5560 - acc: 0.7795\n",
      "Epoch 7/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.5418 - acc: 0.7795\n",
      "Epoch 8/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.5302 - acc: 0.7795\n",
      "Epoch 9/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.5218 - acc: 0.7795\n",
      "Epoch 10/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.5140 - acc: 0.7795\n",
      "Epoch 11/240\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.5073 - acc: 0.7795\n",
      "Epoch 12/240\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.5021 - acc: 0.7795\n",
      "Epoch 13/240\n",
      "712/712 [==============================] - 0s 237us/step - loss: 0.4970 - acc: 0.7795 0s - loss: 0.5045 - acc: 0.770\n",
      "Epoch 14/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4941 - acc: 0.7795\n",
      "Epoch 15/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4890 - acc: 0.7809\n",
      "Epoch 16/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.4858 - acc: 0.7795\n",
      "Epoch 17/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4809 - acc: 0.7823\n",
      "Epoch 18/240\n",
      "712/712 [==============================] - 0s 165us/step - loss: 0.4778 - acc: 0.7809\n",
      "Epoch 19/240\n",
      "712/712 [==============================] - 0s 233us/step - loss: 0.4751 - acc: 0.7823\n",
      "Epoch 20/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4714 - acc: 0.7809\n",
      "Epoch 21/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4691 - acc: 0.7809\n",
      "Epoch 22/240\n",
      "712/712 [==============================] - 0s 242us/step - loss: 0.4631 - acc: 0.7893\n",
      "Epoch 23/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4631 - acc: 0.7837\n",
      "Epoch 24/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4606 - acc: 0.7907\n",
      "Epoch 25/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.4603 - acc: 0.7907\n",
      "Epoch 26/240\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.4539 - acc: 0.7921\n",
      "Epoch 27/240\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.4536 - acc: 0.7907\n",
      "Epoch 28/240\n",
      "712/712 [==============================] - 0s 238us/step - loss: 0.4509 - acc: 0.7921\n",
      "Epoch 29/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4493 - acc: 0.8104\n",
      "Epoch 30/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4497 - acc: 0.7949\n",
      "Epoch 31/240\n",
      "712/712 [==============================] - 0s 237us/step - loss: 0.4475 - acc: 0.8006\n",
      "Epoch 32/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4459 - acc: 0.7935\n",
      "Epoch 33/240\n",
      "712/712 [==============================] - 0s 164us/step - loss: 0.4441 - acc: 0.8048\n",
      "Epoch 34/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.4411 - acc: 0.7935 0s - loss: 0.3852 - acc: 0.81\n",
      "Epoch 35/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4396 - acc: 0.8006\n",
      "Epoch 36/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4386 - acc: 0.8048\n",
      "Epoch 37/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4389 - acc: 0.7963\n",
      "Epoch 38/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.4375 - acc: 0.8006\n",
      "Epoch 39/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4327 - acc: 0.8146\n",
      "Epoch 40/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4334 - acc: 0.7963 0s - loss: 0.4408 - acc: 0.787\n",
      "Epoch 41/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4347 - acc: 0.8048\n",
      "Epoch 42/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4321 - acc: 0.8020\n",
      "Epoch 43/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.4301 - acc: 0.8034\n",
      "Epoch 44/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.4308 - acc: 0.8062\n",
      "Epoch 45/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4310 - acc: 0.8062 0s - loss: 0.4198 - acc: 0.831\n",
      "Epoch 46/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4300 - acc: 0.8090\n",
      "Epoch 47/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.4284 - acc: 0.8048\n",
      "Epoch 48/240\n",
      "712/712 [==============================] - 0s 165us/step - loss: 0.4303 - acc: 0.8132\n",
      "Epoch 49/240\n",
      "712/712 [==============================] - 0s 233us/step - loss: 0.4264 - acc: 0.8146\n",
      "Epoch 50/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.4293 - acc: 0.8132\n",
      "Epoch 51/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4252 - acc: 0.8160\n",
      "Epoch 52/240\n",
      "712/712 [==============================] - 0s 233us/step - loss: 0.4260 - acc: 0.8174\n",
      "Epoch 53/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4247 - acc: 0.8118\n",
      "Epoch 54/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.4256 - acc: 0.8132\n",
      "Epoch 55/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4231 - acc: 0.8160\n",
      "Epoch 56/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.4246 - acc: 0.8202\n",
      "Epoch 57/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4240 - acc: 0.8090\n",
      "Epoch 58/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.4218 - acc: 0.8230 0s - loss: 0.4168 - acc: 0.826\n",
      "Epoch 59/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.4209 - acc: 0.8258\n",
      "Epoch 60/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4203 - acc: 0.8244\n",
      "Epoch 61/240\n",
      "712/712 [==============================] - 0s 245us/step - loss: 0.4215 - acc: 0.8272 0s - loss: 0.4242 - acc: 0.821\n",
      "Epoch 62/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4206 - acc: 0.8188 0s - loss: 0.4099 - acc: 0.819\n",
      "Epoch 63/240\n",
      "712/712 [==============================] - 0s 167us/step - loss: 0.4186 - acc: 0.8202\n",
      "Epoch 64/240\n",
      "712/712 [==============================] - 0s 234us/step - loss: 0.4204 - acc: 0.8258\n",
      "Epoch 65/240\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.4161 - acc: 0.8174\n",
      "Epoch 66/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4174 - acc: 0.8202\n",
      "Epoch 67/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4168 - acc: 0.8272\n",
      "Epoch 68/240\n",
      "712/712 [==============================] - 0s 177us/step - loss: 0.4168 - acc: 0.8230\n",
      "Epoch 69/240\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.4169 - acc: 0.8244\n",
      "Epoch 70/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4136 - acc: 0.8371\n",
      "Epoch 71/240\n",
      "712/712 [==============================] - 0s 166us/step - loss: 0.4175 - acc: 0.8230 0s - loss: 0.4387 - acc: 0.810\n",
      "Epoch 72/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.4146 - acc: 0.8160 0s - loss: 0.4174 - acc: 0.822\n",
      "Epoch 73/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.4135 - acc: 0.8202\n",
      "Epoch 74/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4171 - acc: 0.8216\n",
      "Epoch 75/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4138 - acc: 0.8202 0s - loss: 0.4411 - acc: 0.802\n",
      "Epoch 76/240\n",
      "712/712 [==============================] - 0s 237us/step - loss: 0.4117 - acc: 0.8315\n",
      "Epoch 77/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.4155 - acc: 0.8104\n",
      "Epoch 78/240\n",
      "712/712 [==============================] - 0s 175us/step - loss: 0.4163 - acc: 0.8146\n",
      "Epoch 79/240\n",
      "712/712 [==============================] - 0s 220us/step - loss: 0.4178 - acc: 0.8244\n",
      "Epoch 80/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 160us/step - loss: 0.4118 - acc: 0.8343\n",
      "Epoch 81/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.4124 - acc: 0.8244\n",
      "Epoch 82/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4144 - acc: 0.8188\n",
      "Epoch 83/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4141 - acc: 0.8174\n",
      "Epoch 84/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4124 - acc: 0.8230\n",
      "Epoch 85/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.4132 - acc: 0.8244\n",
      "Epoch 86/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4101 - acc: 0.8272\n",
      "Epoch 87/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4064 - acc: 0.8385\n",
      "Epoch 88/240\n",
      "712/712 [==============================] - 0s 242us/step - loss: 0.4118 - acc: 0.8272\n",
      "Epoch 89/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4100 - acc: 0.8301\n",
      "Epoch 90/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.4116 - acc: 0.8272\n",
      "Epoch 91/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.4060 - acc: 0.8315\n",
      "Epoch 92/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4126 - acc: 0.8315 0s - loss: 0.3888 - acc: 0.843\n",
      "Epoch 93/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.4099 - acc: 0.8301\n",
      "Epoch 94/240\n",
      "712/712 [==============================] - 0s 243us/step - loss: 0.4082 - acc: 0.8272\n",
      "Epoch 95/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4074 - acc: 0.8343\n",
      "Epoch 96/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4075 - acc: 0.8287 0s - loss: 0.4090 - acc: 0.852\n",
      "Epoch 97/240\n",
      "712/712 [==============================] - 0s 245us/step - loss: 0.4065 - acc: 0.8329\n",
      "Epoch 98/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4089 - acc: 0.8315\n",
      "Epoch 99/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4108 - acc: 0.8272\n",
      "Epoch 100/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.4046 - acc: 0.8258\n",
      "Epoch 101/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4081 - acc: 0.8216\n",
      "Epoch 102/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4065 - acc: 0.8287\n",
      "Epoch 103/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4075 - acc: 0.8258\n",
      "Epoch 104/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4079 - acc: 0.8272\n",
      "Epoch 105/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.4069 - acc: 0.8301\n",
      "Epoch 106/240\n",
      "712/712 [==============================] - 0s 236us/step - loss: 0.4083 - acc: 0.8258\n",
      "Epoch 107/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4046 - acc: 0.8202\n",
      "Epoch 108/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.4063 - acc: 0.8343\n",
      "Epoch 109/240\n",
      "712/712 [==============================] - 0s 235us/step - loss: 0.4036 - acc: 0.8315\n",
      "Epoch 110/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4040 - acc: 0.8287\n",
      "Epoch 111/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4016 - acc: 0.8343\n",
      "Epoch 112/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4039 - acc: 0.8371 0s - loss: 0.4523 - acc: 0.79\n",
      "Epoch 113/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.4012 - acc: 0.8329\n",
      "Epoch 114/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4025 - acc: 0.8272\n",
      "Epoch 115/240\n",
      "712/712 [==============================] - 0s 236us/step - loss: 0.4030 - acc: 0.8301\n",
      "Epoch 116/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.3971 - acc: 0.8357\n",
      "Epoch 117/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4022 - acc: 0.8315\n",
      "Epoch 118/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4026 - acc: 0.8343\n",
      "Epoch 119/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4032 - acc: 0.8244\n",
      "Epoch 120/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.4002 - acc: 0.8371\n",
      "Epoch 121/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.4004 - acc: 0.8287\n",
      "Epoch 122/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4026 - acc: 0.8371\n",
      "Epoch 123/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.4040 - acc: 0.8301\n",
      "Epoch 124/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.3996 - acc: 0.8315\n",
      "Epoch 125/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4007 - acc: 0.8413\n",
      "Epoch 126/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.4006 - acc: 0.8287\n",
      "Epoch 127/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.4003 - acc: 0.8287\n",
      "Epoch 128/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3970 - acc: 0.8441\n",
      "Epoch 129/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3999 - acc: 0.8315\n",
      "Epoch 130/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.4026 - acc: 0.8301\n",
      "Epoch 131/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.4023 - acc: 0.8258\n",
      "Epoch 132/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3964 - acc: 0.8357\n",
      "Epoch 133/240\n",
      "712/712 [==============================] - 0s 238us/step - loss: 0.4038 - acc: 0.8230\n",
      "Epoch 134/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3980 - acc: 0.8385\n",
      "Epoch 135/240\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.3979 - acc: 0.8371\n",
      "Epoch 136/240\n",
      "712/712 [==============================] - 0s 243us/step - loss: 0.4010 - acc: 0.8343\n",
      "Epoch 137/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.4016 - acc: 0.8343\n",
      "Epoch 138/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3972 - acc: 0.8385\n",
      "Epoch 139/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.3999 - acc: 0.8343\n",
      "Epoch 140/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3956 - acc: 0.8301\n",
      "Epoch 141/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.3990 - acc: 0.8287\n",
      "Epoch 142/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.3957 - acc: 0.8385\n",
      "Epoch 143/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3929 - acc: 0.8329\n",
      "Epoch 144/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3974 - acc: 0.8329 0s - loss: 0.4109 - acc: 0.839\n",
      "Epoch 145/240\n",
      "712/712 [==============================] - 0s 238us/step - loss: 0.3935 - acc: 0.8357\n",
      "Epoch 146/240\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.3931 - acc: 0.8329\n",
      "Epoch 147/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3976 - acc: 0.8413\n",
      "Epoch 148/240\n",
      "712/712 [==============================] - 0s 242us/step - loss: 0.3937 - acc: 0.8272\n",
      "Epoch 149/240\n",
      "712/712 [==============================] - 0s 167us/step - loss: 0.3944 - acc: 0.8427\n",
      "Epoch 150/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3933 - acc: 0.8357\n",
      "Epoch 151/240\n",
      "712/712 [==============================] - 0s 228us/step - loss: 0.3971 - acc: 0.8301\n",
      "Epoch 152/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3953 - acc: 0.8413\n",
      "Epoch 153/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.3951 - acc: 0.8357 0s - loss: 0.3874 - acc: 0.835\n",
      "Epoch 154/240\n",
      "712/712 [==============================] - 0s 163us/step - loss: 0.3955 - acc: 0.8399\n",
      "Epoch 155/240\n",
      "712/712 [==============================] - 0s 227us/step - loss: 0.3965 - acc: 0.8455\n",
      "Epoch 156/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3943 - acc: 0.8272\n",
      "Epoch 157/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3963 - acc: 0.8230\n",
      "Epoch 158/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.3948 - acc: 0.8371\n",
      "Epoch 159/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3890 - acc: 0.8399\n",
      "Epoch 160/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 158us/step - loss: 0.3936 - acc: 0.8343\n",
      "Epoch 161/240\n",
      "712/712 [==============================] - 0s 242us/step - loss: 0.3974 - acc: 0.8315\n",
      "Epoch 162/240\n",
      "712/712 [==============================] - 0s 165us/step - loss: 0.3933 - acc: 0.8413\n",
      "Epoch 163/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3915 - acc: 0.8483\n",
      "Epoch 164/240\n",
      "712/712 [==============================] - 0s 229us/step - loss: 0.3947 - acc: 0.8385\n",
      "Epoch 165/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3939 - acc: 0.8371\n",
      "Epoch 166/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3926 - acc: 0.8258\n",
      "Epoch 167/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.3931 - acc: 0.8357 1s - loss: 0.2569 - acc: 0.93\n",
      "Epoch 168/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.3913 - acc: 0.8413 0s - loss: 0.3865 - acc: 0.837\n",
      "Epoch 169/240\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.3955 - acc: 0.8385\n",
      "Epoch 170/240\n",
      "712/712 [==============================] - 0s 236us/step - loss: 0.3912 - acc: 0.8315\n",
      "Epoch 171/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3902 - acc: 0.8343\n",
      "Epoch 172/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3946 - acc: 0.8272\n",
      "Epoch 173/240\n",
      "712/712 [==============================] - 0s 238us/step - loss: 0.3924 - acc: 0.8357\n",
      "Epoch 174/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3888 - acc: 0.8315\n",
      "Epoch 175/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.3884 - acc: 0.8385\n",
      "Epoch 176/240\n",
      "712/712 [==============================] - 0s 242us/step - loss: 0.3923 - acc: 0.8357\n",
      "Epoch 177/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3945 - acc: 0.8371\n",
      "Epoch 178/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.3935 - acc: 0.8427 0s - loss: 0.3842 - acc: 0.845\n",
      "Epoch 179/240\n",
      "712/712 [==============================] - 0s 243us/step - loss: 0.3915 - acc: 0.8357\n",
      "Epoch 180/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3922 - acc: 0.8301\n",
      "Epoch 181/240\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.3888 - acc: 0.8399\n",
      "Epoch 182/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.3914 - acc: 0.8343\n",
      "Epoch 183/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3906 - acc: 0.8343\n",
      "Epoch 184/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3914 - acc: 0.8399\n",
      "Epoch 185/240\n",
      "712/712 [==============================] - 0s 236us/step - loss: 0.3878 - acc: 0.8301\n",
      "Epoch 186/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3879 - acc: 0.8371\n",
      "Epoch 187/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3887 - acc: 0.8441\n",
      "Epoch 188/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3873 - acc: 0.8272\n",
      "Epoch 189/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3876 - acc: 0.8413\n",
      "Epoch 190/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3901 - acc: 0.8301\n",
      "Epoch 191/240\n",
      "712/712 [==============================] - 0s 163us/step - loss: 0.3902 - acc: 0.8413 0s - loss: 0.3985 - acc: 0.828\n",
      "Epoch 192/240\n",
      "712/712 [==============================] - 0s 235us/step - loss: 0.3863 - acc: 0.8427\n",
      "Epoch 193/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3893 - acc: 0.8357\n",
      "Epoch 194/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.3892 - acc: 0.8371\n",
      "Epoch 195/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.3895 - acc: 0.8357\n",
      "Epoch 196/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3860 - acc: 0.8483\n",
      "Epoch 197/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3875 - acc: 0.8371\n",
      "Epoch 198/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.3896 - acc: 0.8427\n",
      "Epoch 199/240\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.3875 - acc: 0.8357\n",
      "Epoch 200/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.3866 - acc: 0.8385\n",
      "Epoch 201/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.3886 - acc: 0.8371\n",
      "Epoch 202/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.3869 - acc: 0.8343\n",
      "Epoch 203/240\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.3853 - acc: 0.8413\n",
      "Epoch 204/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.3899 - acc: 0.8371 0s - loss: 0.3932 - acc: 0.835\n",
      "Epoch 205/240\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.3872 - acc: 0.8272\n",
      "Epoch 206/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3891 - acc: 0.8315\n",
      "Epoch 207/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.3868 - acc: 0.8301 0s - loss: 0.4016 - acc: 0.805 - ETA: 0s - loss: 0.3828 - acc: 0.831\n",
      "Epoch 208/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.3870 - acc: 0.8287\n",
      "Epoch 209/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3850 - acc: 0.8469\n",
      "Epoch 210/240\n",
      "712/712 [==============================] - 0s 237us/step - loss: 0.3838 - acc: 0.8441\n",
      "Epoch 211/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3852 - acc: 0.8469\n",
      "Epoch 212/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.3879 - acc: 0.8413 0s - loss: 0.3696 - acc: 0.848\n",
      "Epoch 213/240\n",
      "712/712 [==============================] - 0s 236us/step - loss: 0.3864 - acc: 0.8385\n",
      "Epoch 214/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3840 - acc: 0.8413\n",
      "Epoch 215/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3866 - acc: 0.8357\n",
      "Epoch 216/240\n",
      "712/712 [==============================] - 0s 236us/step - loss: 0.3876 - acc: 0.8483\n",
      "Epoch 217/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.3875 - acc: 0.8385\n",
      "Epoch 218/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3827 - acc: 0.8413\n",
      "Epoch 219/240\n",
      "712/712 [==============================] - 0s 244us/step - loss: 0.3834 - acc: 0.8455\n",
      "Epoch 220/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3865 - acc: 0.8357\n",
      "Epoch 221/240\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.3857 - acc: 0.8385\n",
      "Epoch 222/240\n",
      "712/712 [==============================] - 0s 247us/step - loss: 0.3855 - acc: 0.8469 0s - loss: 0.2960 - acc: 0.85\n",
      "Epoch 223/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.3831 - acc: 0.8399\n",
      "Epoch 224/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.3826 - acc: 0.8427\n",
      "Epoch 225/240\n",
      "712/712 [==============================] - 0s 238us/step - loss: 0.3846 - acc: 0.8315\n",
      "Epoch 226/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.3841 - acc: 0.8272\n",
      "Epoch 227/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3827 - acc: 0.8315\n",
      "Epoch 228/240\n",
      "712/712 [==============================] - 0s 241us/step - loss: 0.3838 - acc: 0.8315\n",
      "Epoch 229/240\n",
      "712/712 [==============================] - 0s 167us/step - loss: 0.3873 - acc: 0.8329\n",
      "Epoch 230/240\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.3837 - acc: 0.8413\n",
      "Epoch 231/240\n",
      "712/712 [==============================] - 0s 233us/step - loss: 0.3833 - acc: 0.8329\n",
      "Epoch 232/240\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.3849 - acc: 0.8371 0s - loss: 0.3712 - acc: 0.853\n",
      "Epoch 233/240\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.3814 - acc: 0.8413\n",
      "Epoch 234/240\n",
      "712/712 [==============================] - 0s 238us/step - loss: 0.3831 - acc: 0.8329\n",
      "Epoch 235/240\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.3849 - acc: 0.8441\n",
      "Epoch 236/240\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.3830 - acc: 0.8371\n",
      "Epoch 237/240\n",
      "712/712 [==============================] - 0s 239us/step - loss: 0.3818 - acc: 0.8469\n",
      "Epoch 238/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 159us/step - loss: 0.3849 - acc: 0.8357\n",
      "Epoch 239/240\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.3823 - acc: 0.8427\n",
      "Epoch 240/240\n",
      "712/712 [==============================] - 0s 240us/step - loss: 0.3811 - acc: 0.8357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc55514e518>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_scaled, y_scaled, batch_size= batch_size, epochs=number_of_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final_pred = model.predict_classes(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"titanic_results_10_nn.csv\"\n",
    "override_writing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final[\"PassengerId\"] = test.index\n",
    "y_final[\"Survived\"] = pd.Series(y_final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_final.to_csv(file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
